---
title: 'Ling 334: Homework 2'
author: 'Instructor: Klinton Bicknell'
date: "Due: April 28, 2015"
output:
  html_document:
    highlight: pygments
---

_Turning it in._ You'll turn in your short answers in PDF form via Canvas. Your code will be 'turned in' by putting it in the `~/ling334/hw2/` folder, and running the command `chmod -R g+r ~/ling334/hw2/`{.bash} on the SSCC when you've finished. Remember to list all others you worked with at the top of your assignment. Also remember that you must do your own write-up and your own programming.

For this assignment, we'll develop $n$-gram models using only a small part of the Brown corpus, to make it easier to find bugs and understand what's going on. The methods that you've learned at this point cannot be naively applied to very large datasets, so **please do not try to run any of these models on the full Brown corpus. It might crash the server for everyone.**

## Unigram model

1. _Creating the `word_to_index` dictionary_ [Coding only: save code as `problem1.py`]
  
    The first step in building an $n$-gram model is to create a dictionary that maps words to indices (which we'll use to access the elements corresponding to that word in a vector or matrix of counts or probabilities). You'll create this dictionary from a vocabulary file that lists each word in the corpus exactly once:
    ```python
    nltk.data.path[0] + '/corpora/brown/brown_vocab_100.txt'
    ```
    This file lists the 813 word types used in the first 100 sentences of the Brown corpus and contains one word per line. We want to create a dictionary in python that maps each word to its order of occurrence in this file, starting with 0. For example, the first word in the file is 'all' and this should map to 0. The last word is 'resolution' and this should map to 812. To create the dictionary in python, you'll
iterate over each line (which is a word plus a newline character), use `rstrip()`{.python} to remove the trailing newline character, and then add the word to the dictionary mapped to its appropriate index. To do this, you'll need to keep track of which line of the file you're in using `enumerate()`{.python}.

    After creating this dictionary, write the dictionary to a file called `word_to_index_100.txt`. Because `wf.write()`{.python} only writes strings, you will need to convert the dictionary to a string before you can write it using the `str()`{.python} function. To check that you've done this correctly, verify in the file output that 'all' is mapped to 0 and 'resolution' to 812.

2. _Building an MLE unigram model_ [Coding and written answer: save code as `problem2.py`]

    Now you'll build a simple MLE unigram model from the first 100 sentences in the Brown corpus, found here:
    ```
    /sscc/home/k/kob734/nltk_data/corpora/brown/brown_100.txt
    ```
    This corpus is represented as one sentence per line with a space separating all words. You'll need to split the sentences into a list of words (e.g., using the string's `.split()`{.python} member function), convert each word to lowercase (e.g., using the string's `.lower()`{.python} member function) and add the end-of-sentence word `</s>'.

    First, copy your code from problem 1 to load the dictionary mapping words to indices. Then `import numpy as np`{.python} and initialize the numpy vector of counts with zeros. Finally, iterate through the sentences and increment counts for each of the words they contain.

    Now print the counts vector. (Do this in the python interpreter. Don't add this printing to your script.) _Q: About how many of the words occurred only once in the corpus. Do you think the proportion of words that occur only once would be higher or lower if we used a larger corpus (e.g., all 57000 sentences in Brown)?_

    Finally, to normalize your counts vector and create probabilities, you can simply divide the counts vector by its sum in numpy like so:
    ```python
    probs = counts / np.sum(counts)
    ```
    Write your new probs vector to a file called `unigram_probs.txt` and verify that the first probability in it (the probability of 'all') is 0.0004223 and that the last probability (probability of 'resolution') is 0.00380068. (Note that our model trained on this small corpus has estimated that 'resolution'
is about 10 times as frequent as 'all'! Models trained on very small corpora are very noisy.)
